# 吴恩达视频学习笔记-第五周

[TOC]

## 一、目标检测

## 1.检测算法

##### 1.1 目标定位

定位分类问题不仅要求判断出图片中物体的种类，还要在图片中标记出它的具体位置，用**边框（Bounding Box，或者称包围盒）**把物体圈起来。一般来说，定位分类问题通常只有一个较大的对象位于图片中间位置；而在目标检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。

如果想定位图像中car的位置，除了让神经网络输出softmax类预测分类外，还可以让神经网络多输出几个单元即输出一个边界框(bounding box)，标记为bx,by,bh,bw，这四个数字是被检测对象的边界框的参数化表示。

![1659493553504](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659493553504.png)

##### 1.2 地址检测

利用神经网络进行对象定位，即通过输出四个参数值bx,by,bh,bw，给出图像中对象的边界框。神经网络可以通过输出图像中**特征点(landmarks)**的坐标来实现对目标特征的识别。如下图,选定人脸的64个landmarks，并生成包含这些landmarks的标签训练集，然后利用神经网络输出脸部关键landmarks的位置。批量添加神经网络的输出单元，用以输出要识别的各个landmarks的坐标值。

![1659493668123](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659493668123.png)

##### 1.3 目标检测

如下图，假设你想构建一个car检测算法，步骤为：

①创建一个标签(label)训练集，X和Y表示适当剪切的car图像样本(在训练集上，你一开始可以使用适当剪切的图像，就是整张图像X几乎都被汽车占据)

②有了训练集，就可以开始训练卷积网络了。输入这些适当剪切过的图像，卷积网络输出Y，0或1表示图像中有car或没有car 

训练完这个卷积神经网络，可以采用**基于滑动窗口的目标检测（Sliding Windows Detection）**算法。步骤如下：

①选择大小适宜的窗口与合适的固定步幅，对测试图片进行从左到右、从上倒下的滑动遍历。每个窗口区域使用已经训练好的 CNN 模型进行识别判断。

②可以选择更大的窗口，然后重复第二步的操作

![1659517870317](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659517870317.png)

滑动窗口目标检测优缺点：

优点：原理简单，且不需要人为选定目标区域 

缺点：需要人为直观设定滑动窗口的大小和步幅。①滑动窗口过小或过大，步幅过大均会降低目标检测的正确率。②每次滑动都要进行一次 CNN 网络计算，如果滑动窗口和步幅较小，计算成本往往很大。 

所以，滑动窗口目标检测算法虽然简单，但是性能不佳，效率较低。 

##### 1.4 在CNN中实现滑动窗口

为了构建滑动窗口的卷积应用，首先要将神经网络的全连接层转化成卷积层。

如下图，假设对象检测算法输入一个14x14x3的图像，使用16个filter，大小为5x5，在filter处理后输出为10x10x16，然后通过2x2的最大池化(pooling)操作使图像减少到5x5x16，然后添加一个连接400个单元的全连接层，接着再添加一个全连接层，最终通过softmax输出Y，用4个数字来表示Y，这四个分类可以是人、car、背景或其它对象：

![1659530619792](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659530619792.png)

针对第一个全连接层，我们可以用5x5的filter来实现，数量是400个，输出是1x1x400。再添加另外一个卷积层，filter的大小是1x1，数量是400。最后经由1x1的filter的处理得到一个softmax激活值。 

通过卷积实现滑动窗口对象检测算法，如下图，假设向滑动窗口卷积网络输入14x14x3的图像。

测试集图像是16x16x3，在最初的滑动窗口算法中，你会把这片蓝色区域输入卷积网络，生成1或0分类，接着滑动窗口，步幅为2个像素，向右滑动2个像素，将这个绿框区域输入给卷积网络，运行整个卷积网络，得到另外一个标签0或1，依次将其它框输入卷积网络。我们在这个16x16x3的图像上滑动窗口，卷积网络运行了4次，于是输出了4个标签(labels)。 

结果发现，这4次卷积操作中的很多计算都是重复的。滑动窗口的卷积应用，使得卷积网络在这4次操作过程中很多计算都是重复的。如果直接将这个16x16x3的图像输入网络，输出是2x2x4，而不是1x1x4。

![1659530871404](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659530871404.png)

所以该卷积操作的原理是我们不需要把输入图像分割成4个子集分别执行前向操作，而是把它们作为一张图像输入给卷积网络进行计算，其中的公有区域可以共享很多计算。

假如对一个28x28x3的图像应用滑动窗口操作，如果以同样的方式运行前向操作(forward prop)，最后得到8x8x4的结果。我们可以对大小为28x28x3的整张图像进行卷积操作，一次得到所有预测值，如果足够幸运，神经网络便可以识别出car的位置。 

但该算法依旧存在缺点，即边界框的位置可能不够准确。

##### 1.5 边界框预测

在滑动窗口法中，取这些离散的位置集合，然后在它们上运行分类器，在这种情况下，这些边界框没有一个能完美匹配汽车位置。

因此推出了YOLO算法， **YOLO(You only look once)**意思是你只看一次，这是由Joseph Redmon，Santosh Divvala，Ross Girshick和Ali Farhadi提出的算法。该算法的做法如下：

如果你的输入图像是100×100的，然后在图像上放一个网格，简单起见，可以使用3x3的网格，在实际使用过程中则可能使用19x19的网格。基本思路是使用**图像分类**和**定位算法**，逐一应用在图像的9个格子中。

![1659533860970](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659533860970.png)

对于9个格子中的每一个指定，都指定一个标签y，y是8维的，而y的表达如下：

![1659534649672](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659534649672.png)

参数解释如下：

- pc 等于0或1取决于这个绿色格子中是否有图像

- bx，by，bh，bw表示： 如果那个格子里有对象，那么就给出边界框坐标 

- c1，c2，c3表示：想要识别的三个类别


下面对图像进行如下标号：

![1659535142105](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659535142105.png)

对于①号，没有任何目标物，则标签向量y为：

![1659535324607](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659535324607.png)

即不包含任何目标物，所以②号框③号框，以及其他什么也没有的格子，其结果都与上述相同。

而当一个目标物同时出现在两个框中，YOLO算法的做法是：**分别取两个对象的中点，然后将这个对象分配给包含对象中点的格子**。所以左边的车被分往④号框，右边的车被分往⑥号框，所以即使⑤号框同时有两辆车的一部分，我们就假装中心格子没有任何我们感兴趣的对象，其标签y和①号框相似。

而对于④号框，y标签就是如下格式：

- 有一个对象，那么pc=1
- 写出bx，by，bh和bw来指定边界框的位置
- 类别1是行人，所以c1=0；类别2是汽车，所以c2=1；类别3是摩托车，所以c3=0

表示方式如下：

![1659608815547](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659608815547.png)

右边格子的结果类似：

![1659608928519](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659608928519.png)

所以对于这里9个格子中任何一个，你都会得到一个8维输出向量，因为这里是3×3的网格，所以有9个格子，总的输出尺寸是3×3×8，所以目标输出是3×3×8。

对于这个例子中，左上格子是1×1×8，对应的是9个格子中左上格子的输出向量。以对于这3×3中每一个位置而言，对于这9个格子，每个都对应一个8维输出目标向量y， 但是其中一些值可能是**dont care-s(即?)**。

如上面的示例中，如果将输入的图片划分为 3×3 的网格、需要检测的目标有 3 类，则每一网格部分图片的标签会是一个 8 维的列矩阵，最终输出的就是大小为 3×3×8 的结果。要得到这个结果，就要训练一个输入大小为 100×100×3，输出大小为 3×3×8 的 CNN。在实践中，可能使用更为精细的 19×19 网格，则两个目标的中点在同一个网格的概率更小。

YOLO算法的优点：

- 和图像分类和目标定位算法类似，显式输出边框坐标和大小，不会受到滑窗分类器的步长大小限制。
- 仍然只进行一次 CNN 正向计算，效率很高，甚至可以达到实时识别。

在指定边界框时，bx，by，bh，bw单位是相对于格子尺寸的比例，所以**bx，by必须介于0与1之间**，因为从定义上看，橙色点位于对象分配到格子的范围内，如果它不在0和1之间，如果它在方块外，那么这个对象就应该分配到另一个格子上。这时**bh和bw可能会大于1**，如这种情况：

![1659618086624](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659618086624.png)

##### 1.6 并交比

**交互比（IoU, Intersection Over Union）**函数用于评价对象检测算法，它计算预测边框和实际边框交集（I）与并集（U）之比，如下图所示： 

![1659619454216](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659619454216.png)

红色方框为真实目标区域，蓝色方框为检测目标区域。两块区域的交集为绿色部分，并集为紫色部分。蓝色方框与红色方框的接近程度可以用IoU比值来定义：

![1659619572523](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659619572523.png)

IoU 的值在 0～1 之间，且越接近 1 表示目标的定位越准确。IoU 大于等于 0.5 时，一般可以认为预测边框是正确的，当然也可以更加严格地要求一个更高的阈值。一般约定，0.5是阈值，用来判断预测的边界框是否正确。  

##### 1.7 非极大值抑制

YOLO 算法中，可能有很多网格检测到同一目标。**非极大值抑制（Non-max Suppression）**会通过清理检测结果，找到每个目标中点所位于的网格，确保算法对每个目标只检测一次。

举例说明，当我们对网格19x19，即361个格子上都运行一次图像检测和定位算法，那么**可能很多格子**都会举手说我的pc值很高，这个格子里有车的概率很高，而不是361个格子中仅有两个格子会报告它们检测出一个对象。所以当你运行算法的时候，最后**可能会对同一个对象做出多次检测**。

进行非极大值抑制的步骤如下：

①将包含目标中心坐标的可信度pc小于阈值（例如 0.6）的网格丢弃；

②选取拥有最大pc的网格:

选取pc最大的网格，然后就说这是最可靠的检测，所以我们就用高亮标记，就说我这里找到了一辆车

③分别计算该网格和其他所有网格的 IoU，将 IoU 超过预设阈值的网格丢弃；

非极大值抑制就会逐一审视剩下的矩形，所有和这个最大的边框有很高交并比，高度重叠的其他边界框，那么这些输出就会被抑制 

④重复第 2~3 步，直到不存在未处理的网格。

所以这就是非极大值抑制，非最大值意味着你只输出概率最大的分类结果，但抑制很接近，但不是最大的其他预测结果，所以这方法叫做非极大值抑制。 

并且，上述步骤适用于单类别目标检测。进行多个类别目标检测时，对于每个类别，应该单独做一次非极大值抑制。 

##### 1.8 锚框

到目前为止，我们讨论的情况都是一个网格只检测一个对象。如果要将算法运用在多目标检测上，需要用到锚框。一个网格的标签中将包含多个锚框，相当于存在多个用以标识不同目标的边框。 

![1659622727279](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659622727279.png)

在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个锚框。输出的标签结果大小从 3×3×8 变为 3×3×16。若两个pc都大于预设阈值，则说明检测到了两个目标。 

在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入锚框进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 IoU 值的该网格的锚框。

锚框也有局限性，对于同一网格有三个及以上目标，或者两个目标的锚框高度重合的情况处理不好。 

并且锚框的形状一般通过人工选取，可以用K-Means对两类对象形状聚类。

##### 1.9 YOLO

构造训练集，假设你要**训练一个算法去检测三种对象**，行人、汽车和摩托车，你还需要显式指定完整的背景类别。这里有3个类别标签，如果你要用两个锚框，那么输出y就是3×3×2×8。

其中，3×3表示3×3个网格，2是锚框的数量，8是向量维度。要构造训练集，需要先遍历9个格子，然后构成对应的目标向量y。

对于如下图示：

![1659670441027](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659670441027.png)

先看看第一个格子（编号1），里面没什么有价值的东西，行人、车子和摩托车，三个类别都没有出现在左上格子中，所以对应那个格子目标 y就是这样的：

![1659670473941](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659670473941.png)

第一个锚框的pc值为0，因为没什么和第一个锚框有关的；第二个锚框的pc值也为0，剩下的值为一些dont care-s值。

而编号2的格子，因为有目标物的出现，因此结果为：

![1659670853337](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659670853337.png)

所以你这样遍历9个格子，遍历3×3网格的所有位置，你会得到这样一个向量，得到一个16维向量，所以最终输出尺寸就是3×3×16。和之前一样，简单起见，我在这里用的是3×3网格，实践中用的可能是19×19×16，或者需要用到更多的锚框，可能是19×19×5×8，即19×19×40，用了5个锚框。

这就是训练集，然后你训练一个卷积网络，输入是图片，可能是100×100×3，然后你的卷积网络最后输出尺寸是，在我们例子中是3×3×16或者3×3×2×8。 

接着，运行非极大值抑制，如果你使用两个锚框，那么对于9个格子中任何一个都会有两个预测的边界框，其中一个的概率pc很低。但9个格子中，每个都有两个预测的边界框，然后对一些框进行抛弃：

![1659671250770](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659671250770.png)

最后，如果你有三个对象检测类别，你希望检测行人，汽车和摩托车，那么你要做的是，对于每个类别单独运行非极大值抑制，处理预测结果所属类别的边界框，用非极大值抑制来处理行人类别，用非极大值抑制处理车子类别，然后对摩托车类别进行非极大值抑制，运行三次来得到最终的预测结果。 

##### 1.10 区域推荐网络

前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，**R-CNN（Region CNN，带区域的 CNN）**被提出。即区域推荐网络，通过对输入图片运行**图像分割算法**，在不同的色块上找出**候选区域（Region Proposal）**，就只需要在这些区域上运行分类器。

![1659671876549](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659671876549.png)

R-CNN 的缺点是运行速度很慢，所以有一系列后续研究工作改进。例如 Fast R-CNN（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、Faster R-CNN（使用卷积对图片进行分割）。不过大多数时候还是比 YOLO 算法慢。 

##### 1.11 U-Net语义分割

语义分割也是应用非常广泛的一项CV任务。相较于只把物体框出来的目标检测，语义分割会把每一类物体的每个像素都精确地标出来。如下图的示例所示，输入一张图片，语义分割会把每一类物体准确地用同一种颜色表示：

![1659672035822](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659672035822.png)

而语义分割的结构过程，就是输入图片后，输出是一个单通道图片，图片的数字表示像素的类别：

![1659672154159](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659672154159.png)

在分类模型中，图像会越卷越小，最后压平放进全连接层并输出多个类别的分类概率。而在语义分割模型中，由于模型的输出也是一幅图像，在输入图像被卷小了以后，应该还有一个放大的过程：

![1659672346785](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659672346785.png)

##### 1.12 反卷积

反卷积和卷积的输入输出大小彻底相反，例如卷积核尺寸为3x3，填充p为1，步长为2。我们会在输出图像上做填充，并且每次在输出图像上一步一步移动。我们把正卷积的输出大小计算公式套到反卷积上的输出上，就能算出反卷积的输入的大小。 

在卷积时，我们是把卷积核与图像对应位置的数字乘起来，再求和，算出一个输出值；反卷积则是反了过来，把一个输入值乘到卷积核的每个位置上，再把乘法结果放再输出的对应位置上。一趟反卷积计算如下图所示： 

![1659681149960](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659681149960.png)

重叠部分，则进行数值的相加

##### 1.13 U-Net架构

U-Net除了对图像使用了先缩小再放大的卷积外，还使用了一种跳连（不是ResNet中残差连接的跳连，而是把两份输入拼接在了一起）。 

这样，在反卷积层中，不仅有来自上一层的输入，还有来自前面相同大小的正卷积的结果。这样做的好处是，后半部分的网络既能获得前一个卷积的抽象、高级（比如类别）的输入，又能获得前半部分网络中具体，低级的特征（比如形状）。这样，后面的层能够更好地生成输出。 

![1659682509147](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659682509147.png)

这幅图中，做运算的图像张量被表示成了一个二维矩形，矩形的高度是图像的宽高，矩形的宽度是通道数。U-Net的前半部分和常见的CNN一样，缩小图像大小，增大图像通道数。而在后半部分中，每次上采样时，一半的通道来自上一层的输出，另一半的通道来自于网络前半部分。 

从图中能看出，U-Net的结构图是一个“U”型，因此它才被叫做U-Net。 

## 二、人脸识别和神经风格迁移

### 1.人脸识别

##### 1.1 人脸识别含义

**人脸验证**和**人脸识别**的识别：

- 人脸验证：一般是一个一对一问题，只需要验证输入的人脸图像是否与某个已知的身份信息对应；
- 人脸识别：一个更为复杂的一对多问题，需要验证输入的人脸图像是否与多个已知身份信息中的某一个匹配。

一般来说，由于需要匹配的身份信息更多导致错误率增加，人脸识别比人脸验证更难一些。因为假设人脸验证系统的错误率是1%，那么在人脸识别中，输出分别与K个模板都进行比较，则相应的错误率就会增加，约K%。模板个数越多，错误率越大一些。 

##### 1.2 单样本学习

人脸识别所面临的一个挑战是要求系统只采集某人的一个面部样本，就能快速准确地识别出这个人，即只用一个训练样本来获得准确的预测结果。这被称为**One-Shot 学习**。 

有一种方法是假设数据库中存有 N 个人的身份信息，对于每张输入图像，用 Softmax 输出 N 种标签。然而这种方法的实际效果很差，因为过小的训练集不足以训练出一个稳健的神经网络；并且如果有新的身份信息入库，需要重新训练神经网络，不够灵活。 

因此，我们通过学习一个 Similarity 函数来实现 One-Shot 学习过程。Similarity 函数定义了输入的两幅图像的差异度，其公式如下： 

![1659683497745](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659683497745.png)

这时可以设置一个超参数，作为判断两幅图片是否为同一个人的依据。

##### 1.3 孪生神经网络

实现 Similarity 函数的一种方式是使用**Siamese网络**，它是一种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络。 

![1659684114003](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684114003.png)

如上图所示，将x1，x2分别输入两个相同的卷积网络中，经过全连接层后不再进行 Softmax，得到特征向量f(x(1)),f(x(2))，这时，Similarity 函数就被定义为两个特征向量之差的 L2 范数：  

![1659684259410](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684259410.png)

我们的目标就是利用梯度下降算法，不断调整网络参数，使得属于同一人的图片之间d(x(1),x(2))很小，而不同人的图片之间d(x(1),x(2))很大。 

##### 1.4 Triplet Loss三元组损失

**Triplet 损失函数**用于训练出合适的参数，以获得高质量的人脸图像编码。“Triplet”一词来源于训练这个神经网络需要大量包含 **Anchor（靶目标）、Positive（正例）、Negative（反例）**的图片组，其中 Anchor 和 Positive 需要是同一个人的人脸图像。如下图：

![1659684376823](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684376823.png)

对于一组的三个图片，应该有：

![1659684407096](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684407096.png)

其中，α被称为 **间隔（margin）**， 用于确保不会总是输出零向量（或者一个恒定的值）。

三元组损失函数的定义：

![1659684474132](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684474132.png)

其中，前式的值需要小于等于0，因此取它和 0 的更大值。

通过梯度下降最小化代价函数。

在选择训练样本时，随机选择容易使 Anchor 和 Positive 极为接近，而 Anchor 和 Negative 相差较大，以致训练出来的模型容易抓不到关键的区别。因此，最好的做法是人为**增加 Anchor 和 Positive 的区别，缩小 Anchor 和 Negative 的区别**，促使模型去学习不同人脸之间的关键差异。

##### 1.5 人脸验证和二进制分类

二分类结构也可用于学习参数以解决人脸识别问题。 

其做法是输入一对图片，将孪生神经网络产生的特征向量输入至同一个 Sigmoid 单元，输出 1 则表示是识别为同一人，输出 0 则表示识别为不同的人。 

![1659684769713](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684769713.png)

Sigmoid 单元对应的表达式为： 

![1659684795939](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684795939.png)

其中，wk和b为学习参数。

无论是对于使用三元组损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出 f(x)并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。 

### 2.神经风格迁移

##### 2.1 神经风格迁移含义

**神经风格迁移（Neural style transfer）**将参考风格图像的风格“迁移”到另外一张内容图像中，生成具有其特色的图像。 一般用C表示内容图片，S表示风格图片，G表示生成的图片：

![1659684880845](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659684880845.png)

##### 2.2 深度卷积神经网络学习含义

想要理解如何实现神经风格转换，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。 

一个典型的CNN网络结构如下所示：

![1659685060661](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659685060661.png)

我们通过遍历所有的训练样本，找出使该层激活函数输出最大的 9 块图像区域。可以看出，浅层的隐藏层通常检测出的是原始图像的边缘、颜色、阴影等简单信息。随着层数的增加，隐藏单元能捕捉的区域更大，学习到的特征也由从边缘到纹理再到具体物体，变得更加复杂。 

![1659685180791](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659685180791.png)

##### 2.3 代价函数

神经风格迁移生成图片G的代价函数由两部分组成：C与G的相似程度以及S和G的相似程度：

![1659685338995](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659685338995.png)

其中，α与β是用于控制相似度比重的超参数。

神经风格迁移的基本算法流程：

首先令G为随机像素点，然后使用梯度下降算法，不断修正G的所有像素点，使得J(G)不断减小，从而使G逐渐有C的内容和G的风格，如下图所示。 

![1659685537174](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659685537174.png)

##### 2.4 内容代价函数

上述内容中的代价函数包含一个内容代价部分和风格代价部分。内容代价函数：

![1659685648807](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659685648807.png)

它表示内容图片C和生成图片G之间的相似度，计算过程如下：

①使用一个预训练好的CNN(VGG)；

②选择一个隐藏层 l来计算内容代价。l太小则内容图片和生成图片像素级别相似，l太大则可能只有具体物体级别的相似。因此，l一般选一个中间层；

③α(c)[l],α(G)[l]，为C和G在I层的激活：

![1659686205660](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659686205660.png)

上述两个参数越相似，则代价函数越小，方法就是使用梯度下降算法，不断迭代修正G的像素值。

##### 2.5 风格代价函数

利用CNN网络模型，图片的风格可以定义成**第l层隐藏层不同通道间激活函数的乘积（相关性）**。 

例如我们选取第l层隐藏层，其各通道使用不同颜色标注，如下图所示。因为每个通道提取图片的特征不同，比如1通道（红色）提取的是图片的垂直纹理特征，2通道（黄色）提取的是图片的橙色背景特征。 

![1659686368475](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659686368475.png)

那么计算这两个通道的相关性大小，相关性越大，表示原始图片及既包含了垂直纹理也包含了该橙色背景；相关性越小，表示原始图片并没有同时包含这两个特征。也就是说，**计算不同通道的相关性，反映了原始图片特征间的相互关系，从某种程度上刻画了图片的“风格”。** 

对于风格图像S，选定网络中的第l层，则相关系数以一个gram矩阵的形式表示：

![1659686487252](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659686487252.png)

其中，i和j为第l层的高度和宽度；k和k'为选定的通道；

同理，对于生成图像G，有：

![1659686585986](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659686585986.png)

则代价函数计算公式如下：

![1659686600647](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659686600647.png)

##### 2.6 1维和3维推广

之前我们处理的都是二维图片，实际上卷积也可以延伸到一维和三维数据。 

例如波形数据：

![1659686868433](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659686868433.png)

EKG 数据（心电图）是由时间序列对应的每个瞬间的电压组成，是一维数据。一般来说我们会用 RNN（循环神经网络）来处理，不过如果用卷积处理，则有： 

输入时间序列维度：14 x 1

滤波器尺寸：5 x 1，滤波器个数：16

输出时间序列维度：10 x 16

而对于三维图片的示例，则有：

![1659686939531](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659686939531.png)

输入 3D 图片维度：14 x 14 x 14 x 1

滤波器尺寸：5 x 5 x 5 x 1，滤波器个数：16

输出 3D 图片维度：10 x 10 x 10 x 16

