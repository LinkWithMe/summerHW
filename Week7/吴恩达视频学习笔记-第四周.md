# 吴恩达视频学习笔记-第四周

[TOC]

## 一、卷积神经网络基础

### 1.卷积神经网络

##### 1.1 计算机视觉

一般的计算机视觉问题包括以下几类： 

①图片分类（Image Classification）

②目标检测（Object detection）

③神经风格转换（Neural Style Transfer）

应用计算机视觉时要面临的一个挑战是数据的维度可能会非常大，可能会带来如下后果：

①神经网络结构复杂，数据量相对较少，容易出现过拟合

②所需内存和计算量巨大

因此，使用卷积神经网络CNN。

##### 1.2 边缘探测示例

神经网络的前几层可以检测边缘，然后后面几层可能检测到物体的部分，接下来靠后的一些层可能检测到完整的物体，如下图示例：

![1659240365224](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/1)

常见的边缘检测有两类：**垂直边缘（Vertical Edges）检测**和**水平边缘（Horizontal Edges）检测**。 

图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为 6x6，中间的矩阵被称作**滤波器（filter）**，尺寸为 3x3，卷积后得到的图片尺寸为 4x4，得到结果如下（数值表示灰度，以左上角和右下角的值为例）： 

![1659240495055](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/2)

需要注意的运算规则：

①卷积运算的求解过程是从左到右，由上到下 

②每次在原始图片矩阵中取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，将结果组成一个矩阵

将最右边的矩阵当作图像，那么中间一段亮一些的区域对应最左边的图像中间的垂直边缘： 

![1659240759607](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/3)

不同框架中卷积操作的运算方式：

①在Python 中，卷积用`conv_forward()`表示

②在 Tensorflow 中，卷积用`tf.nn.conv2d()`表示

③在 keras 中，卷积用`Conv2D()`表示。

##### 1.3 更多边缘检测示例

正边、负边其实就是由亮到暗与由暗到亮的区别，即边缘的过渡：

![1659241065056](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/4)

垂直边缘检测和水平边缘检测的滤波器如下所示： 

![1659241191793](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/5)

通过使用不同的过滤器，可以找出垂直或水平的边缘。还有其它过滤器，如Sobel过滤器、Scharr过滤器。一般将垂直过滤器，顺时针翻转90度，就会得到水平过滤器。一般垂直过滤器，左边是正值，中间是0，右边是负值；而一般水平过滤器，上边是正值，中间是0，下边是负值。 

可以把矩阵中的这9个数字，当成9个参数，并且在之后可以学习使用反向传播算法，其目标就是理解这9个参数，通过反向传播，你可以学习另一种滤波器，这种滤波器对于数据的捕捉能力，甚至可以胜过之前任何的滤波器(单纯的水平边缘和垂直边缘)，它可以检测出45度、75度或73度，甚至是任何角度的边缘。

##### 1.4 填充 Padding

通过输入图片的大小以及滤波器的大小，我们可以根据公式计算出输出图片的大小：

![1659241452417](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/6)

这样会带来两个问题：

①每次卷积运算后，输出图片的尺寸缩小；

②原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息

为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行**填充（Padding）**，以增加矩阵的大小。通常将 0 作为填充值。 

通常有两种卷积策略：

![1659242325474](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/7)

Valid卷积意味着不填充(no dapping)，即p=0。Same卷积意味着填充后你的输入大小和输出大小是一样的，即p=(f-1)/2 

在计算机视觉领域，f通常为奇数。原因如下：

①Same卷积中能得到自然数结果

②滤波器有一个便于表示其所在位置的中心点 

##### 1.5 卷积步长

卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置**步长（Stride）**来压缩一部分信息。设置了步长后的输出格式计算如下：

![1659242733726](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/8)

注意公式中有一个向下取整的符号，用于处理商不为整数的情况。向下取整反映着当取原始矩阵的图示蓝框完全包括在图像内部时，才对它进行运算。 

目前为止我们学习的“卷积”实际上被称为**互相关（cross-correlation）**，而非数学意义上的卷积。按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。 

##### 1.6 三维卷积

如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的和作为输出图片的一个像素值。 

![1659243053495](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/9)

**不同通道的滤波器可以不相同。**例如只检测 R 通道的垂直边缘，G 通道和 B 通道不进行边缘检测，则 G 通道和 B 通道的滤波器全部置零。**当输入有特定的高、宽和通道数时，滤波器可以有不同的高和宽，但通道数必须和输入一致。** 

如果想同时检测垂直和水平边缘，或者更多的边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测：

![1659243200041](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/10)

##### 1.7 卷积网络的一层

卷积神经网络的单层结构如下所示：

![1659243394652](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/11)

与之前的卷积过程相比较，卷积神经网络的单层结构多了激活函数和偏移量，计算过程如下：

①加偏移量：

![1659243447810](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/12)

②激活函数：

![1659243486867](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/13)

对于一个 3x3x3 的滤波器，包括偏移量 在内共有 28 个参数。不论输入的图片有多大，用这一个滤波器来提取特征时，参数始终都是 28 个，固定不变。**即选定滤波器组后，参数的数目与输入图片的尺寸无关。**因此，卷积神经网络的参数相较于标准神经网络来说要少得多。这是 CNN 的优点之一。

##### 1.8 卷积神经网络的简单示例

一个简单的CNN模型：

![1659243947681](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/14)

总共分为如下几步：

①输入图像的大小为39x39x3，第一层卷积层用3x3的filter来检测特征，stride为1，padding为0，这层共有10个filters，这层输出将是37x37x10 

②第二层也是卷积层用5x5的filter，stride为2，padding为0，这层共有20个filters，输出将是17x17x20 

③最后一层卷积层，用5x5的filter, stride为2，这层共有40个filters，输出将是7x7x40

④共1960特征，可以将其平滑(flatten)或展开(unroll)成1960个单元，即输出一个长向量，logistic回归或softmax进行计算最终得出神经网络的预测输出

随着神经网络计算深度不断加深，通常开始时图像会较大，高度和宽度会在一段时间内保持一致，然后随着网络深度的加深而逐渐减少，而通道数在增加。在其它许多卷积神经网络中也有相似操作。 

一个典型的卷积网络通常有三种类型的层：一个是卷积层(Convolution)，通常用Conv来标注；一个是池化层(Pooling)，经常叫做POOL；还有一个是全连接层(Fully connected)，用FC表示。虽然仅用卷积层也有可能构建出很好的神经网络，但大部分神经网络架构师依然会添加池化层和全连接层。

##### 1.9 池化层

**池化层**的作用是缩减模型的大小，提高计算速度，同时减小噪声提高所提取特征的稳健性。 

**最大池化（Max Pooling）**：将输入拆分成不同的区域，输出的每个元素都是对应区域中元素的最大值：

![1659253580244](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/15)

池化过程类似于卷积过程，上图所示的池化过程中相当于使用了一个大小f=2的滤波器，且池化步长s=2。卷积过程中的几个计算大小的公式也都适用于池化过程。如果有多个通道，那么就对每个通道分别执行计算过程。 

最大池化的一种直观解释是，元素值较大可能意味着池化过程之前的卷积过程提取到了某些特定的特征，池化过程中的最大化操作使得只要在一个区域内提取到某个特征，它都会保留在最大池化的输出中。 

**平均池化（Average Pooling）**：从取某个区域的最大值改为求这个区域的平均值：

![1659253947313](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/16)

池化过程**有一组超参数，但是没有参数需要学习**，超参数包括滤波器的大小f，步长s，以及池化类别。

输入维度以及输出维度计算公式：

![1659254200238](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/17)

##### 1.10 CNN示例

一个CNN案例如下：

![1659254730804](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/18)

在计算神经网络的层数时，通常只统计具有权重和参数的层，因此池化层通常和之前的卷积层共同计为一层。 FC3 和 FC4 为全连接层，与标准的神经网络结构一致。  

参数变化如下：

![1659254873972](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/19)

随着神经网络的加深，激活值size会逐渐变小，如果激活值size下降太快，也会影响网络性能。 

##### 1.11 卷积意义

卷积过程有效地减少了 CNN 的参数数量，原因有以下两点： 

①参数共享（Parameter sharing）：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。

②稀疏连接（Sparsity of connections）：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。

池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。 

CNN网络的一些优缺点：

①由于 CNN 参数数量较小，所需的训练样本就相对较少，因此在一定程度上不容易发生过拟合现象。 

②并且 CNN 比较擅长捕捉区域位置偏移。即进行物体检测时，不太受物体在图片中位置的影响，增加检测的准确性和系统的健壮性。 

## 二、深度卷积网络模型

### 1.案例探究

##### 1.1 经典网络

###### 1.1.1 LeNet-5

- LeNet-5 针对灰度图像而训练，因此输入图片的**通道数为 1**

- 该模型总共包含了约 6 万个参数，远少于标准神经网络所需

- 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 **CONV layer->POOL layer->CONV layer->POOL layer->FC layer->FC layer->OUTPUT layer**。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用

- 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。

网络结构：

![1659255804998](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/20)

###### 1.1.2 AlexNet

- AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。
- 当用于训练图像和数据集时，AlexNet 能够**处理非常相似的基本构造模块**，这些模块往往包含大量的隐藏单元或数据。

网络结构：

![1659256012133](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/21)

###### 1.1.3 VGG-16

- VGG 又称 VGG-16 网络，16指网络中包含 **16 个卷积层和池化层**，后接全连接层
- **超参数较少**，只需要专注于构建卷积层。
- 结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。
- VGG 需要训练的**特征数量巨大**，包含多达约 1.38 亿个参数。

网络结构如下：

![1659256170030](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/22)

##### 1.2 残差网络

因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。**残差网络（Residual Networks，简称为 ResNets）**可以有效解决这个问题。 

![1659274529975](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/23)

上图的结构被称为**残差块（Residual block）**。通过**捷径（Short cut，或者称跳跃连接，Skip connections）**，可以将al添加到第二个Relu过程中，直接建立al与a(l+2)的隔层联系。

构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。 

在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，**训练错误会先减少，然后开始增多**。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。

残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。

##### 1.3 残差网络使用意义

假设有一个大型神经网络，其输入为X，输出为a[l]。给这个神经网络额外增加两层，输出为a[l+2]。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。

![1659276862980](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/24)

当发生梯度消失时：

![1659277298049](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/25)

因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。残差网络起作用的主要原因就是这些残差块学习恒等式函数非常容易。

将ResNet用于图像识别：

如下图，这是一个普通网络，输入是一张图像，它有很多卷积层，最后输出一个Softmax；如果把它转换成ResNet，只需要添加跳远连接。这个网络有很多层3*3卷积，而且它们大多都是相同的，这就是添加等维特征向量的原因，所有这些都是卷积层而不是全连接层，因为它们是相等的卷积，维度得以保留。

ResNet类似于其它很多网络，也会有很多卷积层，其中偶尔会有池化层或类似于池化层的层。 

![1659278083513](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/26)

##### 1.4 网络中的网络以及1x1卷积

1x1 卷积（1x1 convolution，或称为 Network in Network）指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味着卷积操作等同于乘积操作。 

![1659280881791](https://github.com/LinkWithMe/summerHW/blob/main/Week7/image/27)

而当通道数更多时，1x1 卷积的作用实际上类似**全连接层**的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。 

池化能压缩数据的**高度和宽度**，而 1×1 卷积能压缩数据的**通道数** 。

在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个：

![1659281584308](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659281584308.png)

##### 1.5 Inception网络动机

在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 Inception 网络的作用即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。 

![1659281727202](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659281727202.png)

如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。

在提升性能的同时，Inception 网络有着较大的计算成本。 

图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。为了解决计算量大的问题，可以引入 **1x1 卷积**来减少其计算量。 

对于同一个例子，我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作**瓶颈层（Bottleneck layer）**。只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能：

![1659282164240](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659282164240.png)

##### 1.6 Inception网络

下图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变：

![1659322823224](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659322823224.png)

多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示： 

![1659322873493](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659322873493.png)

黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。 红框其实都是一个Inception模块，绿色框的最后一层为softmax用来作预测。

##### 1.7 MobileNet

MobileNet，顾名思义，这是一种适用于移动(mobile)设备的神经网络。移动设备的计算资源通常十分紧缺，因此，MobileNet对网络的计算量进行了极致的压缩。 

一次卷积的操作过程如下：

![1659323450321](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659323450321.png)

计算量这么大，主要问题出在每一个输出通道都要与每一个输入通道“全连接”上。为此，我们可以考虑让**输出通道只由部分的输入通道决定**。这样一种卷积的策略叫**逐深度可分卷积(Depthwise Separable Convolution)**。 

逐深度可分卷积分为两步：逐深度卷积(depthwise convolution)，逐点卷积(pointwise convolution)。逐深度卷积生成新的通道，逐点卷积把各通道的信息关联起来。 

之前，对三通道图片做卷积，需要3个卷积核分别处理3个通道。而在逐深度卷积中，我们只要1个卷积核。这个卷积核会把输入图像当成三个单通道图像来看待，分别对原图像的各个通道进行卷积，并生成3个单通道图像，最后把3个单通道图像拼回一个三通道图像。也就是说，逐深度卷积只能生成一幅通道数相同的新图像。 

下一步，是逐点卷积，也就是1x1卷积。它用来改变图片的通道数：

![1659687270957](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659687270957.png)

##### 1.8 MobileNet架构

###### 1.8.1 MobileNet v1

13个逐深度可分卷积模块，之后接通常的池化、全连接、softmax。 

![1659326488639](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659326488639.png)

###### 1.8.2 MobileNet v2

改进了残差连接和扩张操作：

![1659328037913](C:\Users\17799\AppData\Roaming\Typora\typora-user-images\1659328037913.png)

残差连接和ResNet一样。在MobileNet v2中，先做一个扩张维度的1x1卷积，再做逐深度卷积，最后做之前的逐点1x1卷积。由于最后的逐点卷积起到的是减小维度的作用，所以最后一步操作也叫做投影。  

##### 1.9 EfficientNet

EfficientNet能根据设备的计算能力，自动调整网络占用的资源。下面这些因素会占用网络资源： 

①图像分辨率

②网络深度

③特征的长度（即卷积核数量或神经元数量）

EfficientNet中，我们可以在这三个维度上缩放网络，动态改变网络的计算量。EfficientNet的开源实现中，一般会提供各设备下的最优参数。 

### 2.ConvNets的建议

##### 2.1 使用开放源码

很多神经网络复杂细致，并充斥着参数调节的细节问题，因而很难仅通过阅读论文来重现他人的成果。想要搭建一个同样的神经网络，查看开源的实现方案会快很多。 

##### 2.2 迁移学习

在“搭建机器学习项目”课程中，迁移学习已经被提到过。计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做**预训练**，然后转换到自己感兴趣的任务上，有助于加速开发。 

对于已训练好的卷积神经网络，可以将所有层都看作是**冻结**的，只需要训练与你的 Softmax 层有关的参数即可。大多数深度学习框架都允许用户指定是否训练特定层的权重。而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。 

上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。

##### 2.3 数据增强

计算机视觉领域的应用都需要大量的数据。当数据不够时，**数据增强（Data Augmentation）**就有帮助。常用的数据扩增包括**镜像翻转、随机裁剪、色彩转换**。 

其中，色彩转换是对图片的 RGB 通道数值进行随意增加或者减少，改变图片色调。另外，**PCA 颜色增强**指更有针对性地对图片的 RGB 通道进行主成分分析（Principles Components Analysis，PCA），对主要的通道颜色进行增加或减少，可以采用高斯扰动做法来增加有效的样本数量。

##### 2.4 计算机视觉状态

两种主要的数据来源：

①被标记的数据

②特征工程

**特征工程（Hand-engineering，又称 hacks）**指精心设计的特性、网络体系结构或是系统的其他组件。特征工程是一项非常重要也比较困难的工作。在数据量不多的情况下，特征工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此特征工程量大幅减少。

另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：

①集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出

②Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均
