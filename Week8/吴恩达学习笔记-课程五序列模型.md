# 吴恩达学习笔记-课程五序列模型

[TOC]

## 一、循环神经网络

### 1.递归神经网络

#### 1.1 序列模型使用原因

输入序列，输出代表这整个序列的类别。它的应用范围很广。如：

①语音识别(speech recognition)：给定一个输入音频片段X，并要求输出片段对应的文字记录Y，这里输入和输出都是序列数据(sequence data)。因为X是按时序播放的音频片段，输出Y是一系列单词。 

②音乐生成(music generation)：只有输出数据Y是序列；输入数据可以是空集，也可以是单一的整数，这个数可能指代你想要生成的音乐风格，也可能是你想要生成的那首曲子的头几个音符。无论怎样，输入X可以是空的或者就是某个数字，然输入Y是一个序列。 

③情感分类(sentiment classification)：输入数据X是序列，输出可以是一个情感等级(数字)

④DNA序列分析

⑤机器翻译(machine translation) ：输入数据X是序列，输出的Y通常也是个序列

⑥视频行为识别(video activity recognition)

⑦命名实体识别(name entity recognition)：可能会给出一个句子，要求识别出句中的人名。  

所有这些问题都可以被称作使用标签数据(X,Y)作为训练集的监督学习。序列问题可以有很多不同的类型，有些问题里输入X和输出数据Y都是序列，但是X和Y有时也会有不一样的长度。在一些问题里，只有X或Y是序列。 

#### 1.2 注释

一些符号的约定和标识：

- 使用x<t>来索引序列中的位置，t意味着它们是时序序列 
- 使用Tx来表示输入序列的长度 
- 使用Ty来表示输出序列的长度 
- x(i)<t>来表示训练样本i的输入序列中第t个元素 
- Tx(i)来表示第i个训练样本的输入序列长度 
- y(i)<t>来表示训练样本i的输出序列中第t个元素 
- Ty(i)来表示第i个训练样本的输出序列长度 

自然语言处理表示序列里一个单独的单词，表示方法如下：

①做一张词表，有时也称为词典(dictionary)，意思是列一列你的表示方法中要用到的单词，如第一个单词是a，第二个单词是aaron，等等，用了10000个单词大小的词典。一般常见的词典大小为30000到50000，有的也会用百万词典；

②用one-hot表示法来表示词典中的每一个单词， x<t>指代句子里的任意词 

如下：

![1660058047878](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/23)

#### 1.3 循环神经网络模型

不能使用标准神经网络的原因：

①输入和输出数据在不同例子中可以有不同的长度；

②一个单纯的神经网络结构并不共享从文本的不同位置上学到的特征。 

因此提出了序列神经网络。

循环神经网络如下图：当它读到句中的第二单词时，假如是x<2>,它不是仅用x<2>就预测出y<2>，它也会输入一些来自时间步(time-step)1的信息。具体而言，时间步1的激活值就会传递到时间步2。然后，在下一个时间步，循环神经网络输入单词x<3>,然后它尝试输出预测结果y<3>，等等。在每一个时间步中，循环神经网络传递一个激活值到下一个时间步中用于计算。 

要开始整个流程，我们在零时刻，需要编造一个激活值，这通常是零向量。也有些研究员会随机用其它方法初始化a<0>,不过使用零向量作为零时刻的伪激活值是最常见的选择。 

![1660102371277](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/24)

前向传播公式如下，通常会选择tanh作为激活函数：

![1660102417850](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/25)

反向传播的计算方向与前向传播基本上是相反的。所有的这些激活项最后都要取决于参数wa和ba.有了a<1>神经网络就可以计算第一个预测输出y<1>,接着到下一个时间步继续计算出y<2>,y<3>等等一直到y<Ty>。为了计算y,你需要参数wy和by,它们被用于y的所有节点。然后为了计算反向传播，还需要一个损失函数，如标准logistic回归损失函数，也叫交叉熵损失函数。 

通过y<1>可以计算对应的损失函数，即第一个时间步的损失函数L<1>，第二个时间步的损失函数L<2>，一直到最后一个时间步的损失函数L<Ty>。 

最后为了计算出总体的损失函数要把它们都加起来。然后通过公式计算出最后的L。反向传播算法需要在相反的方向上进行计算和传递。最后通过导数相关的参数用梯度下降法来更新参数。

单层RNN结构如下所示:

![1660102887247](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/26)

多层RNN结构如下所示,可以看出，RNN循环机制使模型上一时间步产生的结果能够作为下一个时间步输入的一部分，对下一个时间步的输出产生影响，这就是所说的序列信息。这样便可以很好地应用于NLP各项领域.

![1660102903521](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/27)

例如句子”What time is it ?”,这个句子在RNN中的计算过程如下:

首先,What输入进网络,产生输出O1,同时一部分隐层输出送给下一个神经神经元,这样time的输出O2实际上是同时包括了time单词的输入以及what的输入.以此循环,最终”?”的结果包含了自己以及之前的所有单词

![1660102921956](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/28)

传统RNN的内部结构如下所示,比较简单,通过tanh来进行计算

![1660102938607](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/29)

#### 1.4 不同类型的RNNs

主要的RNN类型有如下几种：

①多对多结构：因为输入序列有很多的输入而输出序列也有很多输出。

②多对一结构：有很多的输入，输出只有一个数字，如情绪分级

③一对一结构：类似于一个小型的标准的神经网络，输入x然后得到输出y 

④一对多结构：有一个输入，但是有很多输出，如音乐生成

⑤多对多结构：输入和输出的长度不同，如机器翻译

#### 1.5 语言模型和序列生成

语言模型所做的工作基本是：输入一个句子，准确地说是一个文本序列,y<1>,y<2>,一直到y<Ty>,然后语言模型会估计某个句子序列中各个单词出现的可能性。

建立语言模型时，为了使用RNN建立出这样的模型，首先需要一个训练集，包含一个很大的英文文本语料库，或者其它的你想用于构建模型的语言的语料库。（语料库，是自然语言处理(NLP)的一个专有名词，意思就是很长的或者说数量众多的英文句子组成的文本）。语料库通常通过设置one-hot向量的方式，以及用EOS来标记句子结尾，用UNK来表示未知词语，来进行实现。

接下来，便是建立一个RNN网络进行实验，输入x<1>之后，计算激活项a<1>，a<1>要做的就是通过softmax进行一些预测来计算出第一个词可能会是什么，其结果就是y<1>,这一步其实就是通过softmax层来预测字典中的任意单词，会是第一个词的概率，所以y<1>的输出是softmax的计算结果，它只是预测第一个词的概率，而不去管结果是什么。并且，如果字典大小是10000，那么softmax层可能输出10002种结果，因为加上了**句子结尾和未知词**两个标识。

然后RNN进入下一个时间步，在下一个时间步中，使用激活项a<2>，在这步要做的是计算出第二词会是什么，现在依然传给它正确的第一个词，我们会告诉它第一个词就是Cats，也就是y<1>,这就是为什么x<2>=y<1>,然后在第二个时间步中，输出结果同样经过softmax层进行预测。 

以此类推，直到最后。所以RNN中的每一步都会考虑前面得到的单词，比如给它前3个单词，让它给出下个词的分布，这就是RNN如何学习，从左到右每次预测一个词。 

#### 1.6 对新序列的采样

在你训练一个序列模型之后，要想了解这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。 

如下图，一个序列模型模拟了任意特定单词序列的概率：

第一步要做的就是对你想要模型生成的第一个词进行采样。于是你输入x<1>=0,a<0>=0,现在你的第一个时间步得到的是所有可能的输出，是经过softmax层后得到的概率，然后根据这个softmax的分布进行随机采样。 

softmax分布给你的信息就是第一个词是a的概率是多少，第一个词是aaron的概率是多少，等等。根据向量中这些概率的分布进行采样，这样就能对第一个词进行采样得到y’<1>。  

第二个时间步需要y<1>作为输入，而现在要做的是把刚刚采样得到的y’<1>作为第二个时间步的输入，然后sotfmax层就会预测y’<2>是什么。无论你得到什么样的选择结果都把它传递到下一个时间步，一直这样直到最后一个时间步。 

这就是你如何从你的RNN语言模型中生成一个随机选择的句子。 

根据你实际的需要，你还可以构建一个基于字符的RNN模型。在这种情况下，你的字典仅包含从a到z的字母，可能还会有空格符，还可以有数字0到9，如果想区分大小写字母，还可以再加上大写的字母，还可以看看实际训练集中可能会出现的字符，然后用这些字符组成你的字典。但是这样训练数据就是一个个单独的字符，而NLP的趋势是**基于词汇的语言模型**。 

![1660116266332](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/30)

#### 1.7 RNNs的梯度消失

基本的RNN算法会存在梯度消失的问题。 

基本的RNN不擅长处理长期依赖的问题，如果出现梯度爆炸的问题(导数值很大或出现了NaN)一个解决方法就是用梯度修剪，通过观察梯度向量，如果它大于某个阈值就缩放梯度向量，来保证其不会太大。通常也可以采用GRU和LSTM的方式。

#### 1.8 门控循环单元(GRU)

GRU相比LSTM而言，参数更少，模型更简单，因此少了过拟合的风险，现在也被广泛使用，包裹更新们zt和重置门rt，内部结构以及计算公式如下：

![1660116736258](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/31)

其中，该公式实现了选择性记忆的功能，通过ht-1和ht以及zt来进行实现

![1660116751061](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/32)

公式路线图如下：

![1660116785776](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/33)

首先根据输入的xt和ht-1计算得出rt和zt。rt再结合ht-1和xt计算出当状态下的ht，之后ht和zt计算出结果

#### 1.9 长短期记忆(LSTM)

LSTM有时比GRU更有效。 

LSTM主要思想是设计了一个记忆细胞,具备选择性记忆的功能,可以选择记忆重要信息,过滤掉噪声信息,减轻记忆负担。

LSTM的内部结构如下所示:

![1660116856261](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/34)

LSTM计算公式如下所示：

![1660116868596](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/35)

首先从底部输入来看，输入ht-1和Xt，通过这三个式子计算出门单元值：

![1660116889326](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/36)

然后得到ft与Ct-1对应元素相乘，Ct与it对应元素相乘，这俩的结果相加，对应图中这部分运算：

![1660116936100](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/37)

然后一部分计算Ot，Ct经过tanh生成mt，构成输出ht和yt

![1660116948288](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/38)

#### 1.10 双向RNN

这个模型可以让你序列的某点处不仅可以获取之前的信息还可以获取未来的信息。 

![1660117111145](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/39)

给定一个输入序列x<1>到x<4>,这个序列首先计算前向的a<1>,然后计算前向的a<2>,接着a<3>,a<4>。而反向序列，从a<4>开始，反向进行，计算反向的a<3>,计算完了反向的a<3>后可以用这些激活值计算反向的a<2>, 然后是反向的a<1>。把所有的这些激活值都计算完毕后，就可以预测结果。

这样使得一个中间时间步的预测结果不仅输入了过去的信息还有现在的信息。这一步涉及了前向和反向的传播信息以及未来的信息。 

双向RNN的缺点是需要完整的数据的序列你才能预测任意位置。

#### 1.11 深度RNNs

如下图左侧，一个标准的神经网络，首先是输入x，然后堆叠上隐含层，所以这里应该有激活值，比如说第一层是a^[1]，接着堆叠上下一层，激活值a^[2]，可以再加一层a^[3]，然后得到预测值y帽。深层的RNN网络如上图右侧，还是画这样的网络，然后按时间展开。注意这里的符号，a^[1](0)表示第1层0时刻的激活值，更一般的用a^[l](t)表示第l层第t个时间点的激活值。上图是一个有三个隐层的新的网络。

![1660117942573](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/40)



## 二、自然语言处理和词嵌入

### 1.词嵌入简介

#### 1.1 词表示

词向量技术是将词转化成为稠密向量，并且对于相似的词，其对应的词向量也相近。稠密向量使用数组的数据结构对向量建模，这种结构通常存储普通的向量。即将单词,通过离散化成分量构成一个庞大的稠密矩阵。

#### 1.2 使用词嵌入

如下图这个命名实体识别例子，用词嵌入作为输入训练好的模型：

![1660118614680](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/41)

Sally Johnson是一个种橙子的农民，如果有一个新的输入，Robert Lin是一个种苹果的农民，因为橙子和苹果很相近，那么算法很容易知道Robert Lin也是一个人的名字。 

词嵌入能够达到这种效果其中一个原因就是学习词嵌入的算法会考察非常大的文本集，可以是一亿个单词甚至达到100亿也是合理的。通过读取大量的互联网文本，接下来你可以把这个词嵌入应用到命名实体识别任务中，尽管你只有一个很小的训练集，可以使用迁移学习把你从互联网上免费获得的大量的无标签文本中学习到的知识能够分辨橙子、苹果、榴莲都是水果的知识，然后把这些知识迁移到一个任务中。

#### 1.3 词嵌入的性能

词嵌入还有一个特性它能帮助实现类比推理。类比推理可能不是NLP应用中最重要的存在，不过它能帮助人们理解词嵌入作了什么以及词嵌入能够干什么。

假设用4维的嵌入向量，假如man对于woman，是否可以推论出king对应什么？根据man和woman的向量表示，它们都是gender上的差异，所以得出这种类比推理的结论的方法，得出其对应Queen

![1660120217878](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/42)

类比推理用算法实现时，如下图，比如词嵌入向量在一个300维的空间里，于是man代表的就是空间中的一个点，另外一个单词woman代表空间另一个点，等等。向量man和woman的差非常接近于向量king和queen之间的差值在gender这一维的差。通过方程找到一个使得相似度最大的单词，如果结果理想的话，会找到单词queen。

相似度函数通常采用余弦相似度，或者平方距离、欧式距离来表示。词嵌入的一个显著成果就是可学习的类比关系的一般性。 

![1660120315299](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/43)

#### 1.4 矩阵嵌入

当我们采用算法来学习词嵌入时实际上是学习一个嵌入矩阵。假设我们的字典含有10000个单词，我们要做的就是学习一个嵌入矩阵E，它将是一个300*10000的矩阵，这个矩阵的各列代表的是字典中10000个不同的单词所代表的不同向量。E乘以one-hot向量会得到嵌入向量。  

### 2.学习词嵌入：Word2vec和Glove

#### 2.1 学习词嵌入

建立一个语言模型是学习词嵌入的好方法，建立一个one-hot向量表示这个词，然后生成一个参数矩阵E，然后用E乘以one-hot得到嵌入向量e4343,这一步意味着e4343是由矩阵Ｅ乘以one-hot向量得到的。然后对其它的词也做相同的操作。现在你有许多300维的嵌入向量。我们能做的是把它们全部放进神经网络中，即输入一些上下文然后预测出目标词。如果用一个固定的历史窗口就意味着你可以处理任意长度的句子。

![1660120903400](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/44)

#### 2.2 词转换成向量形式

Word2Vec模型是一种基于局部上下文窗口的方法，

优点:在单词类比任务中表现较好

缺点:因为word2vec在独立的局部上下文窗口上训练，因此难以利用单词的全局统计信息

主要分为两个算法：

①CBOW模型

CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。通过上下文对目标位置词进行预测。

CBOW与Skip-Gram的思想刚好相反,上述已经阐述了计算思路的理解,在此阐述一下CBOW的计算过程:

上下文词语的独热编码输入到输入层

这些词分别乘以同一个矩阵后都得到各自的向量1*N,并取平均

将1*N矩阵乘以矩阵,得到一个1*V的向量

经过softmax后取概率最大词作为预测词

②Skip-Gram模型

skip-gram是给定输入的单词来预测上下文，实际上分为两部分：建立模型和通过模型获取嵌入词向量。建模过程会先基于训练数据构建一个神经网络，当模型训练好后，我们不会用训练好的模型处理新任务，而是需要这个模型通过训练数据所学得的参数，如隐层的权重矩阵等。

skip-gram模型的训练目标：预测文本中某个字周围可能出现的词。

例如:“今天 的 天气 很 好”

窗口长度为5，那么在窗口中心的词即“天气”，我们希望机器能够通过“天气”来预测“今天，的，很，好”这四个词语（称为背景词）。机器只会用数据说话，所以它会将他的预测表示成条件概率形式：P(今天,的,很,好∣天气)

假设四个被预测的之间是相互独立的，那么公式可以变成:

P(今天∣天气)P(的∣天气)P(很∣天气)P(好∣天气)

目标就是尽可能让这个概率尽可能的大。

优化目标函数如下所示:

![img](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/45)

方便演示,设置窗口大小为3,则根据该公式,优化目标计算公式可写为:

![img](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/plus_2) 

由于概率的连乘会导致最终的乘积非常小，因此我们考虑对式子的每一项进行log处理，使得乘积变大的同时还不会改变其单调性。

#### 2.3 负采样

skip-gram模型可以构造一个监督学习任务，把上下文映射到目标词上。但它的缺点就在于softmax计算起来很慢。一种改进过的算法叫做负采样，可以做到与skip-gram模型相似的事情，但是有了一个更加有效的学习算法。 

例如给定一对单词，orange和juice，我们要去预测这是否是一对上下文词----目标词。orange和juice是一对正样本，orange和king是一对负样本。生成这些数据的方式是选择一个上下文词再选一个目标词，作为正样本；然后给定几次用相同的上下文再从字典中选取随机的词，作为负样本(negative example)。

接下来构造一个监督学习问题，其中学习算法输入x，要去预测目标的标签即预测输出y，因此问题就是给定一对词，像orange和juice， 这个算法就是要分辨两种不同的采样方式。这就是如何生成训练集的方法。关于k的选取，小数据集的话，k从5到20比较好，如果数据集很大，k就选的小一点，对于更大的数据集，k就选择2到5。数据集越小，k就越大。在这个例子中用的是k等于4。

![1660121442340](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/46)

#### 2.4 Glove词向量

GloVe模型结合了LSA算法(基于全局矩阵分解的算法)和word2vec算法的优点，即考虑了全局统计信息又利用了局部上下文。

GloVe所要做的就是使其关系开始明确化，假定Xij是单词i在单词j上下文中出现的次数，这里的i和j就和t和c的功能一样，可以认为Xij等同于Xtc,你也可以遍历你的训练集然后数出单词i和单词j在不同的上下文中出现的个数。Xij就是单词i和单词j出现位置相近时或是彼此接近的频率的计数器。

### 3.使用Word嵌入的应用程序

#### 3.1 情感分类

情绪分类任务就是看一段文本然后分辨这个人是否喜欢他们在讨论的这个东西。 

情绪分类的一个最大挑战就是可能标记的训练集没有那么多，但是有了词嵌入，即使只有中等大小的标记的训练集，你也能构建一个不错的情绪分类器。

#### 3.2 词嵌入除偏

词嵌入中一些有关减少或是消除偏见的办法，如种族、性取向方面的偏见。

训练模型所使用的文本，词嵌入能够反映出性别、种族、年龄、性取向等其它方面的偏见，这些偏见都和社会经济状态相关。词嵌入能够轻易学会用来训练模型的文本中的偏见内容，所以算法获取到的偏见内容就可以反映出人们写作中的偏见。以性别为例，首先我们要做的事就是辨别出我们想要减少或想要消除的特定偏见的趋势，接着是中和步，最后一步是均衡步。

![1660121640809](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/47)

## 三、序列模型和注意力机制

### 1.各种序列到序列架构

#### 1.1 基础模型

如果我们想将一个法语句子翻译为英语句子，seq2seq模型，用x<1>一直到x<5>来表示输入句子的单词，然后我们用y<1>到y<6>来表示输出的句子的单词。

训练新的网络有很多方法，建立起RNN网络后，每次只向该网络中输入一个法语单词，将输入序列接收完毕后，这个RNN网络会输出一个向量来代表这个输入序列。之后，你可以建立一个解码网络，它以编码网络的输出作为输入，之后它可以被训练为每次输出一个翻译后的单词，一直到它输出序列的结尾或者句子的结尾标记，这个解码网络的工作就结束了。 

#### 1.2 选择最有可能的句子

机器翻译可以看作是一个条件语言模型。在语言模型中，能够估计句子的可能性，你也可以将它用于生成一个新的句子。 

机器翻译，如图中，用绿色表示encoder网络，用紫色表示decoder网络，decoder网络和语言模型很相似。机器翻译模型其实和语言模型非常相似，不同在于语言模型总是以零向量开始，而encoder网络会计算出一系列向量来表示输入句子而不是以零向量开始。所以把机器翻译叫做条件语言模型。

![1660126582556](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/48)

在通过模型将法语翻译成英文的过程中，通过输入的法语句子，模型将会告诉你各种英文翻译所对应的可能性。你并不是从得到的分布中进行随机取样，而是你要找到一个英语句子y使得条件概率最大化。 

所以在开发机器翻译系统时，你需要作的一件事就是想出一个算法用来找出合适的y值使得该项最大化。而解决这种问题最通用的算法就是集束搜索。 

同时，不推荐使用贪心算法。贪心搜索是一种来自计算机科学的算法，生成第一个词的分布以后，它将会根据你的条件语言模型挑选出最有可能的第一个词，进入你的机器翻译模型中，在挑选出第一个最有可能的第一个词后，它将会继续挑选出最有可能的第二个词，然后继续挑选第三个最有可能的词，这种算法就叫做贪心搜索。但是你真正需要的是一次性挑选出整个单词序列来使得整体的概率最大化。一次仅仅挑选一个词并不是最佳的选择。

#### 1.3 集束搜索

集束搜索过程：

①首先做的就是挑选要输出的英语翻译中的第一个单词，如一个10000个词的词汇表，在束搜索的第一步中评估第一个单词的概率值，束搜索算法会考虑多个选择，束搜索算法会有一个参数B，叫束宽，在这个例子中束宽设成3，意味着束搜索不会只考虑一个可能结果而是一次会考虑3个。 

束搜索算法会把结果存到计算机内存里以便后面尝试用这三个词。如果束宽设的不一样第一个单词的最可能的选择也不一样。 

②针对每个第一个单词考虑第二个单词是什么，在第二步中我们更关心的是要找到最可能的第一个和第二个单词对，所以不仅仅是第二个单词有最大的概率而是第一个第二个单词对有最大的概率。 

③然后用于下一次束搜索，最终这个过程的输出一次增加一个单词，束搜索最终会找到英语句子。如果束宽设为1实际上就变成了贪婪搜索算法。 

束宽为3的示意图：

![1660126922793](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/49)

#### 1.4 细化集束搜索

长度归一化是对集束搜索算法的一个改进。如下图，束搜索就是最大化这个概率，实际中总是记录概率的对数和而不是概率的乘积。 

![1660127001344](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/50)

而对束宽B，B越大则越可能找到好的句子，但是相应的计算代价也更大。在产品中，通常将束宽设置为10。

由于束搜索算法不输出可能性最大的句子，记录着B为前3或者前10种可能。对束搜索算法进行误差分析时，判断是RNN网络还是束搜索导致的问题，先遍历开发集，然后在其中找出算法产生的错误，能够执行误差分析得出束搜索算法和RNN模型出错的比例是多少，你就可以对开发集中的每一个错误例子，尝试确定这些错误是搜索算法出了问题还是RNN模型出了问题。 

![1660127373606](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/51)

#### 1.5 Bleu分数

机器翻译的一个难题在于，对于一个翻译，可能会有多种结果都同样好，这样对于一个翻译系统是不好进行评估的。常见的解决办法是通过一个叫做BLEU得分的东西来解决。如下图,BLEU得分是一个有用的单一实数评估指标，用于评估生成文本的算法，判断输出的结果是否与人工写出的参考文本的含义相似。

![1660127617602](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/52)

#### 1.6 注意力模型

注意力机制（Attention Mechanism）源于对人类视觉的研究。神经注意力机制可以使得神经网络具备专注于其输入（或特征）子集的能力：选择特定的输入。注意力可以应用于任何类型的输入而不管其形状如何。在计算能力有限情况下，注意力机制（attention mechanism）是解决信息超载问题的主要手段的一种资源分配方案，将计算资源分配给更重要的任务。

注意力模型非常适用于机器翻译中的长句子。对于长句子，人工会一边读一边翻译，在神经网络中记忆非常长句子是非常困难的。注意力模型翻译的很像人类一次翻译句子的一部分。

α注意力参数，主要是告诉我们，应该花多少注意力在这些变量上：

![1660128019169](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/53)

### 2.语音识别-音频数据

#### 2.1 语音识别

有一个音频片段x，通过x来自动生成文本y。用一个很大的数据集，可能长达300个或3000个小时。可以将注意力模型或CTC损失函数应用到语音识别系统中，如下图: 

![1660128312084](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/54)

#### 2.2 触发词检测

采用触发词的方式可以快速唤醒我们的系统

通常采用的是RNN标记的方式，在听到触发词之后，将RNN单元标记为1，未听到触发词时标记为0：

![1660128516102](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/55)

但是缺点是它会使训练集不平衡，0会比1多很多。

通过将标记单元由1个转化为多个(某一时间段)，会使1和0的比例稍微平衡一些，可以解决该问题。

## 四、Transformer网络

### 1.Transformers

#### 1.1 Transformer网络直觉

Transformer是一种网络架构，它在NLP领域十分流行。

从RNN到GRU和LSTM，模型对输入数据通过门控有了更好的控制，但同时也增加了计算量，同时其还是顺序模型。而Transformer模型将注意力机制和CNN卷积神经网络联系起来，并行地处理输入数据：

![1660129334379](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/56)

#### 1.2 自注意力机制

自注意力机制要解决的问题是：当神经网络的输入是多个大小不一样的向量，并且可能因为不同向量之间有一定的关系，而在训练时却无法充分发挥这些关系，导致模型训练结果较差。通过某种运算来直接计算得到句子在编码过程中每个位置上的注意力权重；然后再以权重和的形式来计算得到整个句子的隐含向量表示。

首先看self-attention最初始的公式：

![1660129391305](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/57)

括号中，是X乘以X的转置，即 向量的内积。而向量的内积，即表示“一个向量在另一个向量上的投影”。

举例如下所示：

![1660129401637](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/58)

第一行分别与三列进行内积，得到如右的结果。得到的数值越大，即投影值越大，意味着两个向量的相关度越高。而作为词向量的向量，相关度越高就意味着，关注词A的同时，应当关注B也更多。因此，X乘以X的转置所形成的方阵，保存了每个向量和自己与其他向量进行内积运算的结果。而经过softmax的归一化，将数值之和变为一，如下所示：

![1660129428449](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/59)

这个数值也就实现了“注意力的分配”：当我们关注“早”这个字的时候，我们应该分配0.4的注意力（attention）给它本身，剩下0.4的注意力给“上”，最后的0.2的注意力给“好”。

最后，softmax形成的结果与X再相乘，以“早”这一行为例，乘得结果如下所示：

![1660129445180](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/60)

所得的新向量与X的维度相同，灰色向量与绿色向量相比较，灰色向量为"早"字词向量经过注意力机制加权求和之后的表示。相关度其本质是由向量的内积度量的。

懂了该式子之后，再看Q,K,V矩阵，如下：

![1660129463239](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/61)

Q,K,V矩阵，实际上都是X与矩阵的乘积，本质上都是X的线性变化，与先前分析的公式类似。通过设置中间W矩阵的方式，我们可以更新该矩阵的参数，从而提高模型的拟合能力。而“查询向量”，也就是通过上述公式来完成参数的注意力加权。

#### 1.3 多头注意力机制

自注意力机制的缺陷就是：模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置， 因此提出了通过多头注意力机制来解决这一问题。允许注意力机制组合使用查询、键和值的不同的 子空间表示。

多头注意力机制在自注意力机制的基础上，希望捕获序列内各种范围的依赖关系（例如，短距离依赖和长距离依赖）。可以独立学习得到 h 组不同的 线性投影（linear projections）来变换查询、键和值。然后，这 h 组变换后的查询、键和值将并行地进行注意力池化。计算细节和自注意力机制类似，但最后会拼接各头的矩阵来构成大矩阵，算法流程如下：

![1660129488149](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/62)

最终的结果如下：

![1660129497024](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/63)

相比自注意力机制只有一种注意力加权角度而言，多头注意力模型有多种角度，更增加了表达能力。

#### 1.4 Transformer网络

Transformer模型，只用 encoder-decoder 和 attention 机制就能达到很好的效果，最大的优点是可以高效地并行化。 

用于翻译任务的整体结构如下：

![1660130360048](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/64)

Transformer由Encoder和Decoder两部分组成，各自都包含6个block，其工作流程大致如下：

①获取输入句子的每一个单词的表示向量X，X由单词的词向量和单词位置的Embedidng相加得到：

![1660130604370](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/65)

②将得到的单词表示向量矩阵传入Encoder中，经过6个Encoder block后得到句子所有单词的编码信息矩阵C 。其中n是句子中单词的个数，d是表示向量的维度，每一个Encoder block的输入输出维度完全一致：

![1660130718215](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/66)

③将Encoder输出的编码信息矩阵 C传递到 Decoder 中，Decoder 依次会根据当前翻译过的单词 1~ i 翻译下一个单词 i+1，如下图所示。在使用的过程中，翻译到单词 i+1 的时候需要通过 Mask (掩盖) 操作遮盖住 i+1 之后的单词。 

![1660130813530](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/67)

细节上看，Encoder编码器的设计如下：

![1660131293251](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/68)

有两个子层，一个是multi-head attention层，是利用attention学习源句内部的关系。另一个是feed forward层，简单的全连接网络，对每个position的向量分别进行相同的操作，包括两个线性变换和一个ReLU激活函数。

Decoder解码器设计如下：

![1660131343553](https://github.com/LinkWithMe/summerHW/blob/main/Week8/image/69)

有三个子层，其中两个multi-head attention层。下面的attention层是利用self-attention学习目标句内部的关系，之后该层输出与encoder传过来的结果一起输入到上面的attention层，这个attention层并不是 self-attention，而是encoder-decoder attention，用来学习源句与目标句之间的关系。对于这个Attention，query代表decoder上一步的输出，key和value是来自encoder的输出。

最后再经过一个与编码部分类似的feed forward层，就可以得到decoder的输出了。

## 参考资料

注：因为先前学习过RNN相关知识，所以笔记有些内容直接搬运了自己写过的笔记

[1]  [RNN模型解析 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/409485110) 

[2]  [【重温经典】大白话讲解LSTM长短期记忆网络 如何缓解梯度消失，手把手公式推导反向传播_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1qM4y1M7Nv?spm_id_from=333.999.0.0&vd_source=2e2e5482b016884769e4190397c8bfbb) 

[3]  [【重温经典】GRU循环神经网络 —— LSTM的轻量级版本，大白话讲解_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1454y1n7uY?spm_id_from=333.337.search-card.all.click&vd_source=2e2e5482b016884769e4190397c8bfbb) 

[4]  [注意力机制【动手学深度学习v2】_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1264y1i7R1?spm_id_from=333.337.search-card.all.click) 

[5]  [Transformer模型简介_kuokay的博客-CSDN博客_transformer模型](https://blog.csdn.net/qq_45066628/article/details/123771224) 
