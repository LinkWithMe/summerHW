# 视觉SLAM十四讲笔记

[TOC]

## 一、第一讲

### 1.SLAM

SLAM（同时定位与地图构建），是指搭载特定传感器的主体，在没有环境先验信息的情况下，在运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器是相机，那么就称为是视觉SLAM。

## 二、第二讲

### 1.自主运动的两大基本问题

①定位-明白我在什么地方

②建图-了解周围环境

### 2.传感器

准确的定位需要精确的地图，精确的地图来自准确的定位

通过传感器，机器人能够感知外界环境，并且完成定位和建图

传感器的分类：

①机器人本体可以自己携带的传感器：机器人轮式编码器，相机，激光传感器，惯性测试单元(IMU)

②安装于环境中的传感器：导轨、二维码标志

但是环境传感器有一定的限制，例如不是所有地方都有导轨，以及GPS需要卫星信号的定位。相比之下，本体传感器，例如激光、相机等携带式传感器测量的通常都是一些间接的物理量而不是直接的位置数据，使用更加自由，而且使用携带式传感器来完成SLAM也是关注的重点问题。

### 3.相机

SLAM中所使用的相机较为简单，以一定速率采集图像、形成视频。

相机的特点：

①以二维投影形式来记录三维世界的信息

②相机在处理过程中，丢失了一个维度，即距离(深度)

相机的分类：

单目相机Monocular、双目相机(立体相机)Stereo、深度相机RGB-D、全景相机、事件相机

**单目相机(只使用一个摄像头)：**

通过相机的运动形成视差，可以通过很小几帧就可以测量物体相对深度。

优点：

缺点：通过单张图片，无法确定一个物体的真是大小

如图所示：

![1660478840893](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/24)

图像中的人物，可能是一个很大但很远的物体，也可能是一个很近很小的物体。即单目SLAM估计的轨迹和地图将与真实的轨迹和地图相差一个因子，也就是尺度，单凭图像无法确定这个真实尺度，所以称之为**尺度不确定性**。

**双目相机(由两个单目相机组成)：**

通过基线来估计每个像素的空间位置

优点：基线距离越大，能够测量的距离就越远；并且可以运用到室内和室外

缺点：配置与标定较为复杂，深度量程和精度受到双目基线与分辨率限制，计算时非常消耗计算资源，需要GPU和FPGA设备加速用于两部相机来定位。

**深度相机(RGB-D):**

通过红外结构光或者ToF(time of fly)的物理方法测量物体深度信息

优点：相比于双目相机可节省大量的计算资源

缺点：测量范围窄，噪声大，视野小，易受日光干扰，无法测量透射材质，主要用于室内，很难应用在室外

深度相机主要用来三维成像，以及距离的测量

### 4.经典视觉SLAM框架

一个完善的算法框架如下：

![1660486271887](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/25)

流程步骤如下：

①传感器信息读取：在视觉SLAM中主要为相机图像信息的读取和预处理

②前端视觉里程计(VO):视觉里程计的任务是估计相邻图像间相机的运动，以及局部地图的样子，视觉里程计不可避免地会出现累积漂移的问题

③回环检测：用于判断机器人是否到达先前的位置

④后端(非线性)优化：对不同时刻的视觉里程计测量的相机位姿及回环检测的信息进行优化，得到全局一致的轨迹和地图

⑤建图：根据估计的轨迹，建立任务要求对应的地图

### 5.视觉里程计

如图所示，根据相邻帧间的图像估计相机运动：

![1660486627246](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/26)

视觉里程计通过相邻帧间的图像估计相机运动，并恢复场景的空间结构。但是只计算相邻时刻的运动，不关心再往前的信息。但前端过程中必然存在误差，误差会不断累积，形成**累积漂移**(例如，在前端有1m的误差，经过累积，会发现原本直的走廊变成了斜的，而原本90°的直角变成了歪的)。为消除漂移，需要**回环检测**和**后端优化**。

### 6.后端优化

定义：如何处理前端所传噪声的数据，完成①从带有噪声的数据中估计整个系统的状态，②以及这个状态估计的不确定性有多大。称之为最大后验概率估计。

通常来说，前端给后端提供待优化的数据，以及这些数据的初始值。后端负责整体的优化过程，它往往面对的就只有数据。

后端优化反映了SLAM问题的本质：对运动主体自身和周围环境空间不确定性的估计，即状态估计理论，估计状态的均值和不确定性。

### 7.回环检测

作用：主要解决位置估计随时间漂移的问题，通俗理解为，机器人经过一段时间又回到了原点，但是我们的位置估计值没有回到原点，解决该问题。

目标：通过某种手段，让机器人知道“回到原点”这件事情，让机器人具有识别到过的场景的能力，再把位置估计值“拉”回去，如图：

![1660488458008](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/27)

检测手段：判断与之前位置的差异，计算图像间相似性

回环检测后：将所得到的信息告诉后端优化算法，把轨迹和地图调整到符合回环检测结果的样子

### 8.建图

地图主要分为以下两类：

①度量地图(强调精确地表示地图中的位置关系)

常用稀疏与稠密进行分类

稀疏地图：由路标组成的地图

稠密地图：着重于建模所有看到的东西，可用于导航，并且需要耗费大量的储存空间

②拓扑地图(更加强调元素之间的关系)

与图论类似，所生成的地图是一张图，由节点和边组成。例如，只关注A、B点是否连通，而不考虑如何从A到达B，因此不适用于表达较为复杂结构的地图。

### 9.SLAM问题的数学表述

机器人一般是带着传感器在位置环境里运动，而相机通常是在某些时刻采集数据的，所以我们一般研究这些时刻的位置和地图。

一些参数的约定如下：

![1660532677169](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/28)

需要解决的问题：

①如何认识运动，从k-1到k时刻，机器人的位置如何变化

机器人会携带一个测量自身运动的传感器，这个传感器可以测量有关运动的读数，但不一定直接就是位置之差，还可以是加速度、角速度这些信息。

根据传感器读数、运动过程加入的噪声可以建立运动方程：

![1660532976136](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/29)

②如何认识观测，机器人在k时刻xk处观察到了某个路标yj，如何用数学语言进行描述

机器人在xk位置上看到某个路标点yj，产生了一个观测数据zk,j

用数学模型进行描述：

![1660533092718](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/30)

### 10.参数化形式

**运动方程：**

机器人在平面运动，那么位姿(位置+姿态)由两个位置的坐标和一个转角来描述xk=

![1660533414487](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/31)

同时，运动传感器能够测量到机器人在任意两个时间间隔位置和转角的变化量uk=

![1660533458039](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/32)

于是，此时运动方程就可以写成如下形式，这两个参数以及一个噪声变量：

![1660533510721](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/33)

**观测方程：**

观测方程实际上就是“对路标点拍摄后，得到图像中的像素”的过程。

例如机器人携带着一个二维激光传感器(当激光传感器观测一个2D路标点时，可以测到路标点与机器人之间的距离r以及夹角φ)，我们记路标点为y1,y2；位姿为x1，x2，如下：

![1660533780156](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/34)

而观测数据为：

![1660533788981](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/35)

那么，观测方程就可以写为，其中也需要加入噪声：

![1660533810889](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/36)

因此，SLAM的数学问题主要由运动方程和观测方程构成：

![1660533960356](https://github.com/LinkWithMe/summerHW/blob/main/Week9/image/37)

这两个方程描述了最基本的SLAM问题：

当知道**运动测量的读数u**以及**传感器的读数z**时，如何求解**定位问题(估计x)**和**建图问题(估计y)**
